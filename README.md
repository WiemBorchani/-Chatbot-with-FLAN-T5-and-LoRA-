**README: Building a Chatbot with FLAN-T5 and LoRA (Local Representation Alignment) Training**

# Project Overview

This project involves building a chatbot using FLAN-T5, an open-source sequence-to-sequence language model. The model is trained using LoRA (Local Representation Alignment) to improve its performance in understanding and generating human-like responses.

# Getting Started

## Open in Colab

To run this project, open the provided Colab notebook. Ensure that you have access to a GPU for faster training.

## Installing Essential Libraries

```bash
! pip install transformers datasets accelerate peft
```

This command installs necessary Python libraries: `transformers` for Hugging Face models, `datasets` for handling data, `accelerate` for distributed training, and `peft` for LoRA training.

# Loading the Data

The dataset is loaded using the `load_dataset` function from the `datasets` library. This dataset is generated by GPT-4 for building instruction-following Language Models (LLMs) using supervised learning and reinforcement learning.

```python
data = load_dataset("Fredithefish/Instruction-Tuning-with-GPT-4-RedPajama-Chat")
```

# Data Visualization

The loaded dataset is visualized using Pandas. It consists of human-generated instructions and corresponding assistant-generated responses.

```python
df = data["train"].to_pandas()
```

# Building Train/Test Data

The dataset is split into train and test sets, which are then saved in Parquet format.

```python
df[:15000].to_parquet("train.parquet", index=False)
df[15000:].to_parquet("test.parquet", index=False)
```

# Building the Chatbot

The chatbot is built using the FLAN-T5 model, a fine-tuned version of the T5 model. The model is loaded from the Hugging Face model hub.

```python
model_id = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(model_id)
```

# Preprocessing Function

A preprocessing function is defined to tokenize and prepare samples for the sequence-to-sequence model training.

```python
def preprocess_function(sample, padding="max_length"):
    # Tokenize the "Human" text using the provided tokenizer
    # Tokenize the "Assistant" text for labels
    # Adjust labels for max_length padding if specified
    # Include the adjusted labels in the model inputs
    return model_inputs
```

# Tokenizing and Training

The training data is tokenized using the defined preprocessing function. LoRA training is applied to the model.

```python
train_tokenized_dataset = train_data.map(preprocess_function, batched=True, remove_columns=train_data.column_names)
lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q", "v"], lora_dropout=0.1, bias="none", task_type=TaskType.SEQ_2_SEQ_LM)
model = get_peft_model(model, lora_config)
```

# Training the Model

The model is trained using the Seq2SeqTrainer class from the transformers library.

```python
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_tokenized_dataset,
)
trainer.train()
```

# Saving the Model

The trained model is saved, including its tokenizer and base model, and pushed to the Hugging Face model hub.

```python
peft_save_model_id = "lora-flan-t5-large-chat"
trainer.model.save_pretrained(peft_save_model_id, push_to_hub=True)
tokenizer.save_pretrained(peft_save_model_id, push_to_hub=True)
trainer.model.base_model.save_pretrained(peft_save_model_id, push_to_hub=True)
```

# Using the Chatbot

The saved model can be loaded and used to generate responses based on user input.

```python
model = PeftModel.from_pretrained(model, peft_model_id, device_map={"":0}).cuda()
input_ids = tokenizer("your_input_text", return_tensors="pt", truncation=True, max_length=256).input_ids.cuda()
outputs = model.generate(input_ids=input_ids, do_sample=True, top_p=0.9, max_length=256)
print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])
```

<img width="617" alt="image" src="https://github.com/WiemBorchani/-Chatbot-with-FLAN-T5-and-LoRA-/assets/52404192/af5dd234-d3db-4dd2-806e-5ba96e80e1df">
